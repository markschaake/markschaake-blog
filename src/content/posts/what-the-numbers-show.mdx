---
title: "What the Numbers Show"
description: "I've been tracking machine hours, session concurrency, and code rework across my agentic workflows. The most useful thing the data surfaced wasn't about the machines."
pubDate: 2026-02-24
draft: false
tags: ["ai", "engineering", "methodology"]
heroImage: ./what-the-numbers-show.webp
---
import WallVsMachineChart from '../../components/WallVsMachineChart.astro';
import MachineWallRatioChart from '../../components/MachineWallRatioChart.astro';
import RatioVsChurnChart from '../../components/RatioVsChurnChart.astro';

I've been tracking my machine time. Not in the abstract - literal machine hours, measured by hooks in my Claude Code setup that fire events to an API whenever a session starts, a tool executes, or a session ends. A git integration ties commits back to clusters of sessions on the same repo. The whole thing is maybe 150 lines of bash and an API endpoint, took an afternoon to build, and runs quietly in the background while I work.

The ratio of machine hours to wall hours across my projects has been trending toward 1.5 - meaning for every hour of clock time on a project, the machines are running for about ninety minutes. It's a parallelism indicator, not a production measure - it says nothing about what those machine hours produce, only that multiple agents are working simultaneously.

And the ratio understates the actual leverage, because each machine-hour is itself already some multiple of what I'd accomplish alone. A single AI-assisted session levers me anywhere between 1x and 10x depending on the task - routine scaffolding at the high end, novel architecture at the low end. The 1.5 machine:wall ratio sits _on top of_ that per-session leverage. The compounding is real even if the precise number isn't.

Sometimes I'm at the keyboard during that hour, working alongside the agents. Sometimes I'm not there at all - an expedition plan kicked off before lunch, agents executing across worktrees while I'm playing tennis or homeschooling my son. That felt like progress when I first noticed it. It still does. But the number hides more than it reveals.

<WallVsMachineChart />

<MachineWallRatioChart />

---

Right now I have five Claude Code terminal sessions open - some running, some waiting for my next instruction. That itself was an evolution. When I started, I ran one session at a time. Over months I learned to parallelize my own attention, hopping between projects and between modules within a project, maintaining progressive flow in each thread. This is a different kind of parallelism from the orchestrated handoff - it's _my_ concurrency, not just the machines'.

The bursts where the machine:wall ratio spikes above 2 happen when I fully hand off orchestrated execution of plans that were produced in earlier planning sessions. But the steady-state ratio around 1.5 reflects something subtler: multiple sessions running simultaneously because I've learned to hold multiple threads of work in my head at once.

On a given day I might have three projects in flight, each getting that kind of leverage. But even this dissolves under scrutiny, because one of those projects might be in a planning phase where everything is single-threaded and the ratio sits near 1:1, while another is in burst execution with agents working across git worktrees and the ratio spikes.

And planning - the work that produces the most parallelism downstream - is inherently sequential. Two hours designing the architecture and decomposing the work, then four agents execute simultaneously. The planning shows up as low throughput on any dashboard. The execution shows up as the spike. Neither tells the full story alone, and if I were watching only the ratio, I'd be rewarding the wrong phase of the work.

---

Dan Shapiro <a href="https://www.danshapiro.com/blog/2026/01/the-five-levels-from-spicy-autocomplete-to-the-software-factory/" target="_blank" rel="noopener noreferrer">published a framework</a> earlier this year that maps the industry across five levels - from "spicy autocomplete" (the AI suggests the next line, the human accepts or rejects) through increasingly autonomous stages up to the "dark factory" where no human writes or reviews code. The middle levels describe the progression most developers are somewhere inside of: from handing the AI discrete tasks and reviewing everything, through directing agents at the feature level and reviewing PRs, to writing specifications and evaluating outcomes without reading the code at all.

The framework is useful because it gives honest language to a conversation that's been drowning in marketing. What it doesn't give is a way to know which level someone is actually at. Self-assessment is <a href="/posts/from-assisted-to-agentic">demonstrably unreliable</a> - developers consistently misjudge not just the magnitude of AI's effect on their productivity, but the direction.

Shapiro himself notes that every level "feels like you are done" - each stage of adoption feels complete from the inside. In conversations with engineering leaders, I've seen the same thing: teams that describe themselves as further along in agentic leverage than their workflows suggest. The question isn't how productive someone _feels_. It's what the data says about how they're actually working.

---

There's a cycle I've started noticing in my own data that I think of as planning-to-burst. In weeks where I spend more wall time reading, exploring, and writing specifications - sessions heavy on Read and Explore tools with few edits - the following sessions tend to be dense with parallel execution and high commit output. The planning sessions look slow on any throughput metric. They're the highest-leverage hours of the week. An engineer who's learned to invest in the planning phase before touching implementation is working differently from one who starts editing immediately, even if their machine:wall ratios are similar.

Session concurrency is a clear signal. When I started with Claude Code, all my work was a single terminal session - one agent, iterating together. Over months, I've shifted toward orchestration patterns that spawn multiple Claude Code sessions across git worktrees, each tracked independently. Four sessions running in parallel on the same project, each with its own event stream, each producing commits.

That shift - from working with AI to orchestrating AI - maps directly onto Shapiro's progression from the middle levels toward the upper ones, and it shows up in the data as concurrent sessions that the API clusters into session groups. The measurement granularity is the terminal session, not the individual tool call, and that turns out to be the right level - it captures the deliberate parallelism that reflects how someone has decomposed the work, not the incidental parallelism happening inside the agent runtime.

---

These patterns suggest something like a natural progression, visible in the data before anyone reports it:

An engineer early in the transition shows a machine:wall ratio at 1.0 - one session at a time, one project at a time, the agent as a fast pair programmer. This is where most teams start, and where - per <a href="https://www.danshapiro.com/blog/2026/01/the-five-levels-from-spicy-autocomplete-to-the-software-factory/" target="_blank" rel="noopener noreferrer">Shapiro's estimate</a> - about 90% of developers who consider themselves AI-native are still operating. The signal is in what _isn't_ happening: no concurrency, no planning-to-burst cycles, no separation between the human's attention and the machine's execution.

The shift toward orchestration shows up as the ratio lifting above 1.0 - concurrent sessions appearing for the first time, multiple terminal windows working the same codebase or different projects in parallel. The developer is directing agents, not writing code alongside one. Planning phases get longer relative to execution phases, because the developer is learning that investment in decomposition pays off in parallel execution downstream.

Further along, the data shows heavy planning sessions followed by burst execution across multiple parallel sessions - the ratio spikes well above 1.0. Commit density is high during execution phases. Project parallelism becomes a default rather than an exception, and planning constitutes a large fraction of total wall time.

At this point the developer's primary output is specifications and judgment, not code. The machines handle everything between the spec and the commit.

---

The progression above is entirely about throughput and parallelism. A team could look good on every one of those signals while producing code that keeps getting reworked. The machine:wall ratio says nothing about whether the work sticks.

Code churn is the usual way to measure rework - files modified and then modified again within a short window. But calendar-based churn is a blunt instrument in agentic workflows because it can't distinguish iteration from rework. A file edited five times in a single Claude Code session is just the agent converging on a solution. The same file getting revised in a follow-up session the next day means the first session's output didn't hold - the specification was undercooked, the context was insufficient, or the task decomposition was wrong.

Session-relative rework is the sharper metric. The git integration already tracks which commits came from which sessions, so the computation is straightforward: for each file modified in a session, did it get modified again in a subsequent session within seven days? Not every cross-session re-edit is rework - legitimate features span multiple sessions - but persistent revision of recently produced files is a signal worth watching. I want that number minimal and trending down - as models improve, as the harness gets tighter, as the planning process learns what level of specification produces output that survives to the next session.

Even that metric needed its own iteration. My first version treated all cross-session edits equally - a file touched once in a follow-up session counted the same as one touched seven times. But those are different signals. A single re-edit might be a legitimate extension; seven re-edits means the specification didn't hold. Refining the metric to weight by edit frequency sharpened the signal, and the process of refining it was itself a case of the rework the metric was trying to measure - caused not by the agents but by my own insufficient research into what I was actually trying to capture.

A rising machine:wall ratio paired with flat or declining session rework is the signal of genuine progress - more parallel execution, and the output holds up. A rising ratio paired with rising rework means the parallelism is producing work that doesn't survive contact with reality. Agents stepping on each other, specifications undercooked, review too thin to catch problems before they compound. That pattern looks productive on any throughput dashboard. The rework data shows it isn't.

The type of work complicates the signal further. I categorize my work into <a href="/posts/expedition-excursion-errand">expeditions, excursions, and errands</a> - ranging from heavily planned multi-session projects to quick iterative fixes. Errand-level work naturally produces more churn because the thrashing is the solution-crafting process, not a failure of planning. It's the expedition and excursion work - where I invest in decomposition before burst execution - where rework signals something actually went wrong upstream. The metric doesn't distinguish between these categories yet, and until it does, the interpretation requires context I carry in my head.

But the most useful thing the rework data has surfaced so far isn't about the agents at all - it's about me. Watching the numbers and tracing rework instances back to the sessions that produced them gives me something I didn't have before - a feedback loop. I care more about planning now because I can see its real impact downstream, both when I invest in it and when I skip it.

The planning-to-burst cycle should predict this - sessions starting with thorough decomposition producing lower-rework output downstream, sessions that skip planning and go straight to parallel execution producing high throughput and high rework. I don't have enough data yet to confirm the pattern, but it's the hypothesis the measurement is designed to test.

<RatioVsChurnChart />

---

In a team context, these behavioral signals look less like a dashboard and more like a conversation prompt. The shifts are gradual - a machine:wall ratio lifting from 1.0 to 1.2 over three weeks as someone starts running concurrent sessions for the first time. Planning share increasing as the investment in specification starts to precede execution. An engineer whose edit-loop frequency is high - the same files getting revised repeatedly within a session - might be at a different point in the transition than one whose sessions are heavy on planning tools and light on edits. Session rework rate layered on top of the ratio sharpens the picture further - two engineers with identical machine:wall ratios look very different when one has 8% rework and the other has 25%. The data doesn't diagnose anything, but it makes patterns visible that would otherwise stay hidden behind the same commit log.

An engineer whose ratio has sat at 1.0 for weeks might not have made the shift from writing alongside the agent to directing it - or might be deep in a planning phase that hasn't reached execution yet. The same number means different things depending on where someone is in their work. The measurement doesn't replace the conversation. It gives the conversation something concrete to start from.

The tools landscape is moving fast enough that today's instrumentation might not apply to tomorrow's harness. The measurement itself is scaffolding - useful during the specific window where a team is learning to work differently, not permanent infrastructure. What sticks after the scaffolding comes down isn't the dashboard. It's the instincts that developed while the data was making the transition visible.

---

Measurement can't capture the moment someone's relationship to the code shifts - when they stop thinking of themselves as the person who writes it and start thinking of themselves as the person who specifies what should exist. That shift is internal and gradual and doesn't map cleanly to any metric. But its shadows are in the data. The numbers don't cause the shift. They're how you notice it's underway.
