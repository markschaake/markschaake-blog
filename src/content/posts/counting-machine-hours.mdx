---
title: "Counting Machine Hours"
description: "We have frameworks for describing agentic engineering maturity. We don't have good ways to measure it - and self-assessment is broken."
pubDate: 2026-02-20
draft: false
tags: ["ai", "engineering", "methodology"]
heroImage: ./counting-machine-hours.webp
---
import WallVsMachineChart from '../../components/WallVsMachineChart.astro';
import MachineWallRatioChart from '../../components/MachineWallRatioChart.astro';

I've been tracking my machine time. Not in the abstract - literal machine hours, measured by hooks in my Claude Code setup that fire events to an API whenever a session starts, a tool executes, or a session ends. A git integration ties commits back to clusters of sessions on the same repo. The whole thing is maybe 150 lines of bash and an API endpoint, running quietly in the background while I work.

The ratio of machine hours to wall hours across my projects has been trending toward 1.5 - meaning for every hour of clock time on a project, the machines are running for about ninety minutes. It's a parallelism indicator, not a production measure - it says nothing about what those machine hours produce, only that multiple agents are working simultaneously.

And the ratio understates the actual leverage, because each machine-hour is itself already some multiple of what I'd accomplish alone. A single AI-assisted session levers me anywhere between 1x and 10x depending on the task - routine scaffolding at the high end, novel architecture at the low end. The 1.5 machine:wall ratio sits _on top of_ that per-session leverage. The compounding is real even if the precise number isn't.

Sometimes I'm at the keyboard during that hour, working alongside the agents. Sometimes I'm not there at all - an expedition plan kicked off before lunch, agents executing across worktrees while I'm playing tennis or homeschooling my son. That felt like progress when I first noticed it. It still does. But the number hides more than it reveals.

<WallVsMachineChart />

<MachineWallRatioChart />

---

Right now I have five Claude Code terminal sessions open - some running, some waiting for my next instruction. That itself was an evolution. When I started, I ran one session at a time. Over months I learned to parallelize my own attention, hopping between projects and between modules within a project, maintaining progressive flow in each thread. This is a different kind of parallelism from the orchestrated handoff - it's _my_ concurrency, not just the machines'. The bursts where the machine:wall ratio spikes above 2 happen when I fully hand off orchestrated execution of plans that were produced in earlier planning sessions. But the steady-state ratio around 1.5 reflects something subtler: multiple sessions running simultaneously because I've learned to hold multiple threads of work in my head at once.

On a given day I might have three projects in flight, each getting that kind of leverage. But even this dissolves under scrutiny, because one of those projects might be in a planning phase where everything is single-threaded and the ratio sits near 1:1, while another is in burst execution with agents working across git worktrees and the ratio spikes.

And planning - the work that produces the most parallelism downstream - is inherently sequential. Two hours designing the architecture and decomposing the work, then four agents execute simultaneously. The planning shows up as low throughput on any dashboard. The execution shows up as the spike. Neither tells the full story alone, and a team that optimizes for the ratio without understanding the pipeline will reward the wrong behavior.

---

Dan Shapiro <a href="https://www.danshapiro.com/blog/2026/01/the-five-levels-from-spicy-autocomplete-to-the-software-factory/" target="_blank" rel="noopener noreferrer">published a framework</a> earlier this year that maps the industry across five levels - from "spicy autocomplete" (the AI suggests the next line, the human accepts or rejects) through increasingly autonomous stages up to the "dark factory" where no human writes or reviews code. The middle levels describe the progression most developers are somewhere inside of: from handing the AI discrete tasks and reviewing everything, through directing agents at the feature level and reviewing PRs, to writing specifications and evaluating outcomes without reading the code at all.

The framework is useful because it gives honest language to a conversation that's been drowning in marketing. What it doesn't give is a way to know which level someone is actually at. Self-assessment is <a href="/posts/from-assisted-to-agentic">demonstrably unreliable</a> - developers consistently misjudge not just the magnitude of AI's effect on their productivity, but the direction.

Asking a team "what level are you at?" produces answers that skew optimistic by a meaningful margin. The question isn't how productive someone _feels_. It's what the data says about how they're actually working.

---

The measurement infrastructure is lighter than it sounds - shell scripts hooked into Claude Code events, an API endpoint that clusters overlapping sessions on the same repo into groups, and a git integration that maps commits to the sessions that produced them. The whole thing took an afternoon to build and runs in the background without my thinking about it.

---

There's a cycle I've started noticing in my own data that I think of as planning-to-burst. In weeks where I spend more wall time reading, exploring, and writing specifications - sessions heavy on Read and Explore tools with few edits - the following sessions tend to be dense with parallel execution and high commit output. The planning sessions look slow on any throughput metric. They're the highest-leverage hours of the week. An engineer who's learned to invest in the planning phase before touching implementation is working differently from one who starts editing immediately, even if their machine:wall ratios are similar.

Session concurrency is a clear signal. When I started with Claude Code, all my work was a single terminal session - one agent, iterating together. Over months, I've shifted toward orchestration patterns that spawn multiple Claude Code sessions across git worktrees, each tracked independently. Four sessions running in parallel on the same project, each with its own event stream, each producing commits. That shift - from working with AI to orchestrating AI - maps directly onto Shapiro's progression from the middle levels toward the upper ones, and it shows up in the data as concurrent sessions that the API clusters into session groups. The measurement granularity is the terminal session, not the individual tool call, and that turns out to be the right level - it captures the deliberate parallelism that reflects how someone has decomposed the work, not the incidental parallelism happening inside the agent runtime.

---

These patterns suggest something like a natural progression, visible in the data before anyone reports it:

An engineer early in the transition shows a machine:wall ratio at 1.0 - one session at a time, one project at a time, the agent as a fast pair programmer. This is where most teams start, and where - per Shapiro's estimate - about 90% of developers who consider themselves AI-native are still operating. The signal is in what _isn't_ happening: no concurrency, no planning-to-burst cycles, no separation between the human's attention and the machine's execution.

The shift toward orchestration shows up as the ratio lifting above 1.0 - concurrent sessions appearing for the first time, multiple terminal windows working the same codebase or different projects in parallel. The developer is directing agents, not writing code alongside one. Planning phases get longer relative to execution phases, because the developer is learning that investment in decomposition pays off in parallel execution downstream.

Further along, the data shows heavy planning sessions followed by burst execution across multiple parallel sessions - the ratio spikes well above 1.0. Commit density is high during execution phases. Project parallelism becomes a default rather than an exception, and planning constitutes a large fraction of total wall time.

At this point the developer's primary output is specifications and judgment, not code. The machines handle everything between the spec and the commit.

---

In a team context, these behavioral signals look less like a dashboard and more like a conversation prompt. The shifts are gradual - a machine:wall ratio lifting from 1.0 to 1.2 over three weeks as someone starts running concurrent sessions for the first time. Planning share increasing as the investment in specification starts to precede execution. An engineer whose edit-loop frequency is high - the same files getting revised repeatedly within a session - might be at a different point in the transition than one whose sessions are heavy on planning tools and light on edits. The data doesn't diagnose anything, but it makes patterns visible that would otherwise stay hidden behind the same commit log.

An engineer whose ratio has sat at 1.0 for weeks might not have made the shift from writing alongside the agent to directing it - or might be deep in a planning phase that hasn't reached execution yet. The same number means different things depending on where someone is in their work. The measurement doesn't replace the conversation. It gives the conversation something concrete to start from.

The tools landscape is moving fast enough that today's instrumentation might not apply to tomorrow's harness. The measurement itself is scaffolding - useful during the specific window where a team is learning to work differently, not permanent infrastructure. What sticks after the scaffolding comes down isn't the dashboard. It's the instincts that developed while the data was making the transition visible.

---

Measurement can't capture the moment someone's relationship to the code shifts - when they stop thinking of themselves as the person who writes it and start thinking of themselves as the person who specifies what should exist. That shift is internal and gradual and doesn't map cleanly to any metric. But its shadows are in the data. The numbers don't cause the shift. They're how you notice it's underway.
